---
title: "Bagging, Random Forests and Boosting: Exercise"
output: html_notebook
---

This notebook walks you through the exercise for the supervised learning II session.

## Setup

```{r}
library(caret)
library(randomForest)
library(xgboost)
```

In this exercise, we again use the reduced version of the BACH data set.

```{r}
load("BACH.Rda")
```

## Data preparation

Our machine learning task is -- again -- to predict whether a corporation experiences a net loss in the year 2015. Therefore, we first compute a dummy variable which indicates a loss with "1" and returns a "0" otherwise, based on the variable `bach$net_profit_or_loss`.

```{r}

```

Then we split the data set into a training and test part, using the year 2015 for the test set.

```{r}

```

## Random Forest

We may want to grow a random forest as a first classifier, using `caret`. This time, we want to be explicit about our data structure in the Cross-Validation process. For this, take a look at the function `groupKFold` and apply it as needed here.

```{r}

```

The resulting object from `groupKFold` can be passed to the `index` argument of the `trainControl` function. Furthermore, specify Cross-Validation as the evaluation method for model tuning.

```{r}

```

Now we can use `train` from `caret` in order to grow the forest. Use the binary loss variable as the outcome and `~ . - net_profit_or_loss - return_on_equity` on the right hand side of the function call.

```{r}

```

Here we can add some code for inspecting the random forest results.

```{r}

```

### Boosting

We may want to use Boosting as an additional prediction method. When using `xgboost`, it is useful to specify a tuning grid first.

```{r}

```

Now we can pass this grid to `train`, using `xgbTree` as the machine learning method. Many arguments can be copied from the previous call to `train`.

```{r}

```

Again, take a look at the results from the tuning process, e.g. by printing and/or plotting the corresponding object.

```{r}

```

## Prediction

Finally, we can use `predict` in order to predict class membership in the test set based on the results from both classifiers.

```{r}

```

Given predicted class membership, we can use `confusionMatrix` for evaluating prediction performance.

```{r}

```
